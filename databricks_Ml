import numpy as np
import pandas as pd
import sklearn.datasets
import sklearn.metrics
import sklearn.model_selection
import sklearn.ensemble

from hyperopt import fmin, tpe, hp, SparkTrials, Trials, STATUS_OK
from hyperopt.pyll import scope

white_wine = spark.read.csv(
    "dbfs:/databricks-datasets/wine-quality/winequality-white.csv",
    sep=";",
    header=True
)

red_wine = spark.read.csv(
    "dbfs:/databricks-datasets/wine-quality/winequality-red.csv",
    sep=";",
    header=True
)

# Remove the spaces from the column names
for c in white_wine.columns:
    white_wine = white_wine.withColumnRenamed(c, c.replace(" ", "_"))

for c in red_wine.columns:
    red_wine = red_wine.withColumnRenamed(c, c.replace(" ", "_"))

# Define table names
red_wine_table = f"{CATALOG_NAME}.{SCHEMA_NAME}.red_wine"
white_wine_table = f"{CATALOG_NAME}.{SCHEMA_NAME}.white_wine"

# Write to tables in Unity Catalog
spark.sql(f"DROP TABLE IF EXISTS {red_wine_table}")
spark.sql(f"DROP TABLE IF EXISTS {white_wine_table}")

white_wine.write.saveAsTable(
    f"{CATALOG_NAME}.{SCHEMA_NAME}.white_wine"
)

red_wine.write.saveAsTable(
    f"{CATALOG_NAME}.{SCHEMA_NAME}.red_wine"
)

white_wine = spark.read.table(
    f"{CATALOG_NAME}.{SCHEMA_NAME}.white_wine"
).toPandas()

red_wine = spark.read.table(
    f"{CATALOG_NAME}.{SCHEMA_NAME}.red_wine"
).toPandas()

# Add boolean fields for red and white wine
white_wine["is_red"] = 0.0
red_wine["is_red"] = 1.0

data_df = pd.concat([white_wine, red_wine], axis=0)

# Define classification labels based on the wine quality
data_labels = data_df["quality"].astype("int") >= 7
data_df = data_df.drop(["quality"], axis=1)

data_df.display()

# Split 80/20 train-test
X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(
    data_df,
    data_labels,
    test_size=0.2,
    random_state=1
)

mlflow.autolog()

with mlflow.start_run(run_name="gradient_boost") as run:
    model = sklearn.ensemble.GradientBoostingClassifier(
        random_state=0
    )

    # Models, parameters, and training metrics are tracked automatically
    model.fit(X_train, y_train)

    predicted_probs = model.predict_proba(X_test)
    roc_auc = sklearn.metrics.roc_auc_score(
        y_test,
        predicted_probs[:, 1]
    )

    # The AUC score on test data is not automatically logged, so log it manually
    mlflow.log_metric("test_auc", roc_auc)

    print("Test AUC of: {}".format(roc_auc))

model_loaded = mlflow.pyfunc.load_model(
    "runs:/{run_id}/model".format(
        run_id=run.info.run_id
    )
)

predictions_loaded = model_loaded.predict(X_test)
predictions_original = model_2.predict(X_test)

assert np.array_equal(
    predictions_loaded,
    predictions_original
)


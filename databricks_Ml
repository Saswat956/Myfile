import numpy as np
import pandas as pd
import sklearn.datasets
import sklearn.metrics
import sklearn.model_selection
import sklearn.ensemble

from hyperopt import fmin, tpe, hp, SparkTrials, Trials, STATUS_OK
from hyperopt.pyll import scope

white_wine = spark.read.csv(
    "dbfs:/databricks-datasets/wine-quality/winequality-white.csv",
    sep=";",
    header=True
)

red_wine = spark.read.csv(
    "dbfs:/databricks-datasets/wine-quality/winequality-red.csv",
    sep=";",
    header=True
)

# Remove the spaces from the column names
for c in white_wine.columns:
    white_wine = white_wine.withColumnRenamed(c, c.replace(" ", "_"))

for c in red_wine.columns:
    red_wine = red_wine.withColumnRenamed(c, c.replace(" ", "_"))

# Define table names
red_wine_table = f"{CATALOG_NAME}.{SCHEMA_NAME}.red_wine"
white_wine_table = f"{CATALOG_NAME}.{SCHEMA_NAME}.white_wine"

# Write to tables in Unity Catalog
spark.sql(f"DROP TABLE IF EXISTS {red_wine_table}")
spark.sql(f"DROP TABLE IF EXISTS {white_wine_table}")

white_wine.write.saveAsTable(
    f"{CATALOG_NAME}.{SCHEMA_NAME}.white_wine"
)

red_wine.write.saveAsTable(
    f"{CATALOG_NAME}.{SCHEMA_NAME}.red_wine"
)

white_wine = spark.read.table(
    f"{CATALOG_NAME}.{SCHEMA_NAME}.white_wine"
).toPandas()

red_wine = spark.read.table(
    f"{CATALOG_NAME}.{SCHEMA_NAME}.red_wine"
).toPandas()

# Add boolean fields for red and white wine
white_wine["is_red"] = 0.0
red_wine["is_red"] = 1.0

data_df = pd.concat([white_wine, red_wine], axis=0)

# Define classification labels based on the wine quality
data_labels = data_df["quality"].astype("int") >= 7
data_df = data_df.drop(["quality"], axis=1)

data_df.display()

# Split 80/20 train-test
X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(
    data_df,
    data_labels,
    test_size=0.2,
    random_state=1
)

mlflow.autolog()

with mlflow.start_run(run_name="gradient_boost") as run:
    model = sklearn.ensemble.GradientBoostingClassifier(
        random_state=0
    )

    # Models, parameters, and training metrics are tracked automatically
    model.fit(X_train, y_train)

    predicted_probs = model.predict_proba(X_test)
    roc_auc = sklearn.metrics.roc_auc_score(
        y_test,
        predicted_probs[:, 1]
    )

    # The AUC score on test data is not automatically logged, so log it manually
    mlflow.log_metric("test_auc", roc_auc)

    print("Test AUC of: {}".format(roc_auc))

model_loaded = mlflow.pyfunc.load_model(
    "runs:/{run_id}/model".format(
        run_id=run.info.run_id
    )
)

predictions_loaded = model_loaded.predict(X_test)
predictions_original = model_2.predict(X_test)

assert np.array_equal(
    predictions_loaded,
    predictions_original
)

search_space = {
    'n_estimators': scope.int(hp.quniform('n_estimators', 20, 1000, 1)),
    'learning_rate': hp.loguniform('learning_rate', -3, 0),
    'max_depth': scope.int(hp.quniform('max_depth', 2, 5, 1)),
}

def train_model(params):
    # Enable autologging on each worker
    mlflow.autolog()

    with mlflow.start_run(nested=True):
        model_hp = sklearn.ensemble.GradientBoostingClassifier(
            random_state=0,
            **params
        )

        model_hp.fit(X_train, y_train)

        predicted_probs = model_hp.predict_proba(X_test)
        roc_auc = sklearn.metrics.roc_auc_score(
            y_test,
            predicted_probs[:, 1]
        )

        mlflow.log_metric('test_auc', roc_auc)

        return {
            'status': STATUS_OK,
            'loss': -1 * roc_auc
        }

spark_trials = SparkTrials(
    parallelism=1
)

with mlflow.start_run(run_name='gb_hyperopt') as run:
    best_params = fmin(
        fn=train_model,
        space=search_space,
        algo=tpe.suggest,
        max_evals=2,
        trials=spark_trials
    )

best_run = mlflow.search_runs(
    order_by=["metrics.test_auc DESC", "start_time DESC"],
    max_results=10,
).iloc[0]

print("Best Run")
print("AUC: {}".format(best_run["metrics.test_auc"]))
print("Num Estimators: {}".format(best_run["params.n_estimators"]))
print("Max Depth: {}".format(best_run["params.max_depth"]))
print("Learning Rate: {}".format(best_run["params.learning_rate"]))

best_model_pyfunc = mlflow.pyfunc.load_model(
    "runs:/{run_id}/model".format(
        run_id=best_run.run_id
    )
)

best_model_predictions = X_test
best_model_predictions["prediction"] = best_model_pyfunc.predict(X_test)

predictions_table = f"{CATALOG_NAME}.{SCHEMA_NAME}.predictions"

spark.sql(f"DROP TABLE IF EXISTS {predictions_table}")

results = spark.createDataFrame(best_model_predictions)

# Write results back to Unity Catalog from Python
results.write.saveAsTable(
    f"{CATALOG_NAME}.{SCHEMA_NAME}.predictions"
)

model_uri = "runs:/{run_id}/model".format(
    run_id=best_run.run_id
)

mlflow.register_model(
    model_uri,
    f"{CATALOG_NAME}.{SCHEMA_NAME}.wine_model"
)




